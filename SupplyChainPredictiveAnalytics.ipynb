{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#FNN Training"
      ],
      "metadata": {
        "id": "1WSjAdqwKx30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "\n",
        "class FNNModel:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.label_encoders = {}\n",
        "\n",
        "    def preprocess_data(self, df, is_training=True):\n",
        "        # Create a copy of the dataframe to avoid modifying the original\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        # Encode categorical columns\n",
        "        categorical_cols = ['Mode_of_Shipment', 'Product_Importance']\n",
        "        for col in categorical_cols:\n",
        "            if is_training:\n",
        "                le = LabelEncoder()\n",
        "                df_processed[col] = le.fit_transform(df_processed[col])\n",
        "                self.label_encoders[col] = le\n",
        "            else:\n",
        "                df_processed[col] = self.label_encoders[col].transform(df_processed[col])\n",
        "\n",
        "        # Separate features and target\n",
        "        X = df_processed.drop('Reached_on_Time', axis=1)\n",
        "        y = df_processed['Reached_on_Time']\n",
        "\n",
        "        # Scale numerical features\n",
        "        if is_training:\n",
        "            X = self.scaler.fit_transform(X)\n",
        "        else:\n",
        "            X = self.scaler.transform(X)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def build_model(self, input_dim):\n",
        "        self.model = Sequential([\n",
        "            Dense(64, activation='relu', input_dim=input_dim),\n",
        "            Dropout(0.3),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "        self.model.compile(optimizer='adam',\n",
        "                          loss='binary_crossentropy',\n",
        "                          metrics=['accuracy'])\n",
        "\n",
        "    def train(self, train_path, val_path, save_path, epochs=20, batch_size=32):\n",
        "        # Load and preprocess data\n",
        "        train_df = pd.read_csv(train_path)\n",
        "        val_df = pd.read_csv(val_path)\n",
        "\n",
        "        X_train, y_train = self.preprocess_data(train_df, is_training=True)\n",
        "        X_val, y_val = self.preprocess_data(val_df, is_training=False)\n",
        "\n",
        "        # Build model\n",
        "        self.build_model(input_dim=X_train.shape[1])\n",
        "\n",
        "        # Train model\n",
        "        history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "        # Save the model and preprocessing objects\n",
        "        self.save_model(save_path)\n",
        "\n",
        "        return history\n",
        "\n",
        "    def save_model(self, save_path):\n",
        "        \"\"\"Save model and all preprocessing objects\"\"\"\n",
        "        # Save Keras model\n",
        "        self.model.save(save_path)\n",
        "\n",
        "        # Save preprocessing objects\n",
        "        preprocessors = {\n",
        "            'scaler': self.scaler,\n",
        "            'label_encoders': self.label_encoders\n",
        "        }\n",
        "        with open(save_path + '_preprocessors.pkl', 'wb') as f:\n",
        "            pickle.dump(preprocessors, f)\n",
        "\n",
        "    def load_model(self, model_path):\n",
        "        \"\"\"Load model and all preprocessing objects\"\"\"\n",
        "        # Load Keras model\n",
        "        self.model = load_model(model_path)\n",
        "\n",
        "        # Load preprocessing objects\n",
        "        with open(model_path + '_preprocessors.pkl', 'rb') as f:\n",
        "            preprocessors = pickle.load(f)\n",
        "            self.scaler = preprocessors['scaler']\n",
        "            self.label_encoders = preprocessors['label_encoders']\n",
        "\n",
        "    def predict(self, data_path, output_path=None):\n",
        "        # Load and preprocess data\n",
        "        df = pd.read_csv(data_path)\n",
        "        X, _ = self.preprocess_data(df, is_training=False)\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = self.model.predict(X)\n",
        "        df['Predictions'] = (predictions > 0.5).astype(int)\n",
        "\n",
        "        # Save predictions\n",
        "        if output_path:\n",
        "            df.to_csv(output_path, index=False)\n",
        "            print(f\"Predictions saved to {output_path}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Mount Google Drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    absolute_path = '/content/gdrive/My Drive/Projects/SupplyChainPredictiveAnalytics/'\n",
        "\n",
        "    fnn = FNNModel()\n",
        "\n",
        "    # For training:\n",
        "    # fnn.train(\n",
        "    #     train_path=absolute_path+'train_set.csv',\n",
        "    #     val_path=absolute_path+'validation_set.csv',\n",
        "    #     save_path=absolute_path+'fnn_model.h5',\n",
        "    #     epochs=50\n",
        "    # )\n",
        "\n",
        "    # For prediction:\n",
        "    # fnn.load_model(absolute_path+'fnn_model.h5')\n",
        "    # predictions = fnn.predict(\n",
        "    #     data_path=absolute_path+'test_set.csv',\n",
        "    #     output_path=absolute_path+'predictions.csv'\n",
        "    # )\n",
        "    # print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDPai3CpIVj7",
        "outputId": "c14acd20-94d6-42be-d4fc-a2da1ff16947"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Predictions saved to /content/gdrive/My Drive/Projects/SupplyChainPredictiveAnalytics/predictions.csv\n",
            "     Mode_of_Shipment  Customer_Care_Calls  Cost_of_the_Product  \\\n",
            "0                Ship                    3                  166   \n",
            "1                Ship                    3                  242   \n",
            "2                Ship                    5                  167   \n",
            "3                Ship                    4                  219   \n",
            "4                Ship                    5                  245   \n",
            "...               ...                  ...                  ...   \n",
            "1755           Flight                    3                  153   \n",
            "1756             Ship                    2                  149   \n",
            "1757             Ship                    3                  214   \n",
            "1758           Flight                    4                  154   \n",
            "1759             Ship                    3                  255   \n",
            "\n",
            "     Product_Importance  Weight_in_Grams  Reached_on_Time  Predictions  \n",
            "0                medium             5275                1            0  \n",
            "1                  high             4127                0            0  \n",
            "2                   low             4325                0            0  \n",
            "3                medium             3684                1            1  \n",
            "4                   low             1956                1            0  \n",
            "...                 ...              ...              ...          ...  \n",
            "1755                low             1328                1            1  \n",
            "1756             medium             5541                0            0  \n",
            "1757                low             5319                0            0  \n",
            "1758             medium             4361                1            0  \n",
            "1759             medium             4415                0            0  \n",
            "\n",
            "[1760 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering"
      ],
      "metadata": {
        "id": "bk1KVV8rK6Ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from sklearn.metrics import mutual_info_score\n",
        "from google.colab import drive\n",
        "\n",
        "class FeatureAnalyzer:\n",
        "    def __init__(self, df, target_col='Reached_on_Time'):\n",
        "        self.df = df.copy()\n",
        "        self.target_col = target_col\n",
        "        self.numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "        self.categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "    def generate_basic_stats(self):\n",
        "        \"\"\"Generate basic statistics for all columns\"\"\"\n",
        "        stats_dict = {\n",
        "            'total_records': len(self.df),\n",
        "            'on_time_rate': (self.df[self.target_col].mean() * 100).round(2),\n",
        "            'numeric_stats': self.df[self.numeric_cols].describe(),\n",
        "            'categorical_counts': {col: self.df[col].value_counts(normalize=True) * 100\n",
        "                                 for col in self.categorical_cols}\n",
        "        }\n",
        "        return stats_dict\n",
        "\n",
        "    def analyze_numeric_features(self):\n",
        "        \"\"\"Analyze relationship between numeric features and target\"\"\"\n",
        "        numeric_insights = {}\n",
        "\n",
        "        for col in self.numeric_cols:\n",
        "            if col != self.target_col:\n",
        "                # Calculate statistics for on-time vs delayed shipments\n",
        "                on_time_stats = self.df[self.df[self.target_col] == 1][col].describe()\n",
        "                delayed_stats = self.df[self.df[self.target_col] == 0][col].describe()\n",
        "\n",
        "                # Perform t-test\n",
        "                t_stat, p_value = stats.ttest_ind(\n",
        "                    self.df[self.df[self.target_col] == 1][col],\n",
        "                    self.df[self.df[self.target_col] == 0][col]\n",
        "                )\n",
        "\n",
        "                # Calculate correlation\n",
        "                correlation = self.df[col].corr(self.df[self.target_col])\n",
        "\n",
        "                numeric_insights[col] = {\n",
        "                    'on_time_stats': on_time_stats,\n",
        "                    'delayed_stats': delayed_stats,\n",
        "                    't_statistic': t_stat,\n",
        "                    'p_value': p_value,\n",
        "                    'correlation': correlation\n",
        "                }\n",
        "\n",
        "                # Create visualization\n",
        "                plt.figure(figsize=(10, 6))\n",
        "                sns.boxplot(x=self.target_col, y=col, data=self.df)\n",
        "                plt.title(f'Distribution of {col} by Delivery Status')\n",
        "                plt.savefig(absolute_path+ f'boxplot_{col}.png')\n",
        "                plt.close()\n",
        "\n",
        "        return numeric_insights\n",
        "\n",
        "    def analyze_categorical_features(self):\n",
        "        \"\"\"Analyze relationship between categorical features and target\"\"\"\n",
        "        categorical_insights = {}\n",
        "\n",
        "        for col in self.categorical_cols:\n",
        "            # Calculate delivery success rate by category\n",
        "            success_rates = self.df.groupby(col)[self.target_col].mean() * 100\n",
        "\n",
        "            # Calculate chi-square test\n",
        "            contingency_table = pd.crosstab(self.df[col], self.df[self.target_col])\n",
        "            chi2, p_value = stats.chi2_contingency(contingency_table)[:2]\n",
        "\n",
        "            # Calculate mutual information score\n",
        "            mi_score = mutual_info_score(self.df[col], self.df[self.target_col])\n",
        "\n",
        "            categorical_insights[col] = {\n",
        "                'success_rates': success_rates,\n",
        "                'chi2_statistic': chi2,\n",
        "                'p_value': p_value,\n",
        "                'mutual_info_score': mi_score\n",
        "            }\n",
        "\n",
        "            # Create visualization\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            success_rates.plot(kind='bar')\n",
        "            plt.title(f'Delivery Success Rate by {col}')\n",
        "            plt.ylabel('Success Rate (%)')\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(absolute_path+ f'success_rate_{col}.png')\n",
        "            plt.close()\n",
        "\n",
        "        return categorical_insights\n",
        "\n",
        "    def generate_correlation_matrix(self):\n",
        "        \"\"\"Generate correlation matrix for numeric features\"\"\"\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        correlation_matrix = self.df[self.numeric_cols].corr()\n",
        "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
        "        plt.title('Feature Correlation Matrix')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(absolute_path+'correlation_matrix.png')\n",
        "        plt.close()\n",
        "        return correlation_matrix\n",
        "\n",
        "    def generate_summary_report(self):\n",
        "        \"\"\"Generate a comprehensive summary report\"\"\"\n",
        "        basic_stats = self.generate_basic_stats()\n",
        "        numeric_insights = self.analyze_numeric_features()\n",
        "        categorical_insights = self.analyze_categorical_features()\n",
        "        correlation_matrix = self.generate_correlation_matrix()\n",
        "\n",
        "        report = {\n",
        "            'basic_stats': basic_stats,\n",
        "            'numeric_insights': numeric_insights,\n",
        "            'categorical_insights': categorical_insights,\n",
        "            'correlation_matrix': correlation_matrix\n",
        "        }\n",
        "\n",
        "        return report\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    drive.mount('/content/gdrive')\n",
        "    absolute_path = '/content/gdrive/My Drive/Projects/SupplyChainPredictiveAnalytics/'\n",
        "\n",
        "    # Load your dataset\n",
        "    df = pd.read_csv(absolute_path+'test_set.csv')\n",
        "\n",
        "    # Create analyzer instance\n",
        "    analyzer = FeatureAnalyzer(df)\n",
        "\n",
        "    # Generate comprehensive report\n",
        "    report = analyzer.generate_summary_report()\n",
        "\n",
        "    # Print key insights\n",
        "    print(f\"Dataset size: {report['basic_stats']['total_records']} records\")\n",
        "    print(f\"Overall on-time delivery rate: {report['basic_stats']['on_time_rate']}%\")\n",
        "\n",
        "    # Print insights for each numeric feature\n",
        "    for feature, insights in report['numeric_insights'].items():\n",
        "        print(f\"\\nFeature: {feature}\")\n",
        "        print(f\"Correlation with on-time delivery: {insights['correlation']:.3f}\")\n",
        "        print(f\"Statistical significance (p-value): {insights['p_value']:.3f}\")\n",
        "\n",
        "    # Print insights for each categorical feature\n",
        "    for feature, insights in report['categorical_insights'].items():\n",
        "        print(f\"\\nFeature: {feature}\")\n",
        "        print(\"Success rates by category:\")\n",
        "        print(insights['success_rates'])\n",
        "        print(f\"Chi-square p-value: {insights['p_value']:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FXHaAGwK9md",
        "outputId": "9318251b-0bdc-49a9-a1b3-ea02d1f737ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Dataset size: 1760 records\n",
            "Overall on-time delivery rate: 50.0%\n",
            "\n",
            "Feature: Customer_Care_Calls\n",
            "Correlation with on-time delivery: -0.091\n",
            "Statistical significance (p-value): 0.000\n",
            "\n",
            "Feature: Cost_of_the_Product\n",
            "Correlation with on-time delivery: -0.073\n",
            "Statistical significance (p-value): 0.002\n",
            "\n",
            "Feature: Weight_in_Grams\n",
            "Correlation with on-time delivery: -0.252\n",
            "Statistical significance (p-value): 0.000\n",
            "\n",
            "Feature: Mode_of_Shipment\n",
            "Success rates by category:\n",
            "Mode_of_Shipment\n",
            "Flight    52.671756\n",
            "Road      50.175439\n",
            "Ship      49.381698\n",
            "Name: Reached_on_Time, dtype: float64\n",
            "Chi-square p-value: 0.626\n",
            "\n",
            "Feature: Product_Importance\n",
            "Success rates by category:\n",
            "Product_Importance\n",
            "high      52.666667\n",
            "low       50.602410\n",
            "medium    48.846154\n",
            "Name: Reached_on_Time, dtype: float64\n",
            "Chi-square p-value: 0.618\n"
          ]
        }
      ]
    }
  ]
}